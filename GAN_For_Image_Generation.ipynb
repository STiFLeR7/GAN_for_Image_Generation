{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdN8Eb5TotiS",
        "outputId": "7d8287fe-1dba-4d3f-ee9d-75052f115e4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image"
      ],
      "metadata": {
        "id": "iu1IOX1MpCd6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Device Config"
      ],
      "metadata": {
        "id": "fXzxalTEpr0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "knGpIDdGpbxV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "o-_0I1bOpu3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100\n",
        "image_size = 28\n",
        "image_channels = 1\n",
        "batch_size = 64\n",
        "num_epochs = 100\n",
        "learning_rate = 0.0002"
      ],
      "metadata": {
        "id": "M0GMSUBPpp74"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator"
      ],
      "metadata": {
        "id": "0Ir4AL_9qDEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(Generator, self).__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(latent_dim, 128),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Linear(128, 256),\n",
        "        nn.BatchNorm1d(256),\n",
        "        nn.Linear(256, 512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(512, 1024),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(1024, image_channels * image_size * image_size),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "  def forward(self, z):\n",
        "    return self.model(z)\n",
        "    img = img.view(img.size(0), image_channels, image_size, image_size)\n",
        "    return img"
      ],
      "metadata": {
        "id": "8_I7S7WKqEa-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator"
      ],
      "metadata": {
        "id": "yXuLvWWWq8me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(784, 512),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Linear(256, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.model(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "jr2RCiNiq_Ms"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initilizing Generator and Discriminator"
      ],
      "metadata": {
        "id": "plR9fs86sJ4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator(latent_dim).to(device)\n",
        "discriminator = Discriminator().to(device)"
      ],
      "metadata": {
        "id": "CpDX4q_xsNJk"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset and Defining Data Loader"
      ],
      "metadata": {
        "id": "17UBZc_xuj9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataset = datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "zlVc_639vRDz"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function and Optimizers"
      ],
      "metadata": {
        "id": "WJ-RKcIPvzMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas = (0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas = (0.5, 0.999))"
      ],
      "metadata": {
        "id": "t6MBJHkEvywS"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN Training"
      ],
      "metadata": {
        "id": "A1zzzDD9wIV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  for i, (imgs, _) in enumerate(data_loader):\n",
        "    #config_input\n",
        "    real_imgs = imgs.to(device)\n",
        "\n",
        "    # generate real and fake labels\n",
        "  valid = torch.ones((imgs.size(0), 1), requires_grad=False).to(device)\n",
        "  fake = torch.zeros((imgs.size(0),1 ), requires_grad=False).to(device)\n",
        "\n",
        "  optimizer_D.zero_grad()\n",
        "\n",
        "  z = torch.randn((imgs.size(0), latent_dim)).to(device)\n",
        "  gen_imgs = generator(z)\n",
        "\n",
        "  real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "  fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "  d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "  d_loss.backward()\n",
        "  optimizer_D.step()\n",
        "\n",
        "  #training Generator\n",
        "\n",
        "  optimizer_G.zero_grad()\n",
        "\n",
        "  z = torch.randn((imgs.size(0), latent_dim)).to(device)\n",
        "  gen_imgs = generator(z)\n",
        "\n",
        "  g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "  g_loss.backward()\n",
        "  optimizer_G.step()\n",
        "\n",
        "  if i % 100 == 0:\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}] Batch [{i}/{len(data_loader)}] Loss D: {d_loss.item()} Loss G: {g_loss.item()}\")\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      save_image(gen_imgs.data[:25], f\"images/{epoch}.png\", nrow=5, normalize=True)"
      ],
      "metadata": {
        "id": "Zj_5g8E4wKUB"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate and Save Imgs"
      ],
      "metadata": {
        "id": "5f6-uUZmDUUO"
      }
    },
    {
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Create the 'images' directory if it doesn't exist\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "\n",
        "z = torch.randn(64, latent_dim).to(device)\n",
        "gen_imgs = generator(z)\n",
        "save_image(gen_imgs.data, \"images/generated.png\", nrow=8, normalize=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Lh9QUqsuDznp"
      },
      "execution_count": 65,
      "outputs": []
    }
  ]
}